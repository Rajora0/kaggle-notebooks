{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Phi-3-mini-FineTuning\n\n","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers torch datasets peft accelerate bitsandbytes wandb trl huggingface_hub -q","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:07:18.865474Z","iopub.execute_input":"2024-08-08T17:07:18.866206Z","iopub.status.idle":"2024-08-08T17:10:05.482017Z","shell.execute_reply.started":"2024-08-08T17:07:18.866173Z","shell.execute_reply":"2024-08-08T17:10:05.481037Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:10:05.484380Z","iopub.execute_input":"2024-08-08T17:10:05.485083Z","iopub.status.idle":"2024-08-08T17:10:24.758427Z","shell.execute_reply.started":"2024-08-08T17:10:05.485043Z","shell.execute_reply":"2024-08-08T17:10:24.757372Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2024-08-08 17:10:11.572779: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-08 17:10:11.572902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-08 17:10:11.708951: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Phi 3 Mini 4k on Portuguese Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = \"microsoft/Phi-3-mini-4k-instruct\"\ndataset_name = \"dominguesm/Canarim-Instruct-PTBR-Dataset\"\nnew_model = \"phi-3-mini-4k-canarim\"","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:31:49.490615Z","iopub.execute_input":"2024-08-07T19:31:49.491025Z","iopub.status.idle":"2024-08-07T19:31:49.500506Z","shell.execute_reply.started":"2024-08-07T19:31:49.490975Z","shell.execute_reply":"2024-08-07T19:31:49.499177Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:31:49.501874Z","iopub.execute_input":"2024-08-07T19:31:49.502377Z","iopub.status.idle":"2024-08-07T19:31:49.517058Z","shell.execute_reply.started":"2024-08-07T19:31:49.502345Z","shell.execute_reply":"2024-08-07T19:31:49.516254Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:31:53.297099Z","iopub.execute_input":"2024-08-07T19:31:53.297473Z","iopub.status.idle":"2024-08-07T19:32:36.178396Z","shell.execute_reply.started":"2024-08-07T19:31:53.297445Z","shell.execute_reply":"2024-08-07T19:32:36.177361Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef09c32343a94b9c926f1798f175229c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e52a53619840b499005bab5142ed19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be9e71668a7468faf63e0e92cd574a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129f515c4726411fa6b93bb0a2c8a55b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3650ef1beb414f6ca3ad5502e35afd8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220fa737b2814ab99674968e44f8b295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea5ba7328b248edbc41afb3bab7deee"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:32:55.374978Z","iopub.execute_input":"2024-08-07T19:32:55.375803Z","iopub.status.idle":"2024-08-07T19:32:56.849765Z","shell.execute_reply.started":"2024-08-07T19:32:55.375759Z","shell.execute_reply":"2024-08-07T19:32:56.848503Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79110ea1bc324a2e9d07bcd56eb3e498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"743052708c5341ab8f90c7ca5124d3ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de47db37862a4e7e9b5fd0fdd4b82ab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df86a770e024b97a56932e5042b19b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d06b0ea3fcef474293b73f5472def4f5"}},"metadata":{}}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:32:59.168880Z","iopub.execute_input":"2024-08-07T19:32:59.169661Z","iopub.status.idle":"2024-08-07T19:32:59.396500Z","shell.execute_reply.started":"2024-08-07T19:32:59.169630Z","shell.execute_reply":"2024-08-07T19:32:59.395446Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"instruction\"]},\n               {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:33:02.694910Z","iopub.execute_input":"2024-08-07T19:33:02.695314Z","iopub.status.idle":"2024-08-07T19:33:09.811437Z","shell.execute_reply.started":"2024-08-07T19:33:02.695283Z","shell.execute_reply":"2024-08-07T19:33:09.810290Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c77705592ea4feda7dc358a995bebaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/63.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"948c2db55b41426aaee434bc4584078f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/423k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e140432f1b2f4aa5ad9ca5f4107b1734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/316413 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf10b2758024648b493277c86c08e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1519 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffae8052449445687278d7033d6176d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"106ba4c155d149a6aa051eecab5eb120"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nTarefa: Descubra a probabilidade de o seguinte texto ser lido por alguém que não está familiarizado com o tópico. saída alta, média, baixa. Entrada: Uma boa maneira de começar a resolver um problema é pensar sobre o que você já sabe sobre o problema.<|im_end|>\\n<|im_start|>assistant\\nBaixa<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:33:29.648167Z","iopub.execute_input":"2024-08-07T19:33:29.648918Z","iopub.status.idle":"2024-08-07T19:33:29.668190Z","shell.execute_reply.started":"2024-08-07T19:33:29.648880Z","shell.execute_reply":"2024-08-07T19:33:29.667465Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:33:33.708434Z","iopub.execute_input":"2024-08-07T19:33:33.708802Z","iopub.status.idle":"2024-08-07T19:33:33.743122Z","shell.execute_reply.started":"2024-08-07T19:33:33.708773Z","shell.execute_reply":"2024-08-07T19:33:33.742172Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:33:37.143000Z","iopub.execute_input":"2024-08-07T19:33:37.143892Z","iopub.status.idle":"2024-08-07T19:33:37.569547Z","shell.execute_reply.started":"2024-08-07T19:33:37.143856Z","shell.execute_reply":"2024-08-07T19:33:37.566461Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853db368edcc403f9f533c065fba3c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bacbb96c5ccd4bfb99b438b7b4e9906a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:33:42.391248Z","iopub.execute_input":"2024-08-07T19:33:42.392096Z","iopub.status.idle":"2024-08-07T19:42:45.481871Z","shell.execute_reply.started":"2024-08-07T19:33:42.392053Z","shell.execute_reply":"2024-08-07T19:42:45.481039Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\nYou are not running the flash-attention implementation, expect numerical differences.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 08:59, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>1.258800</td>\n      <td>1.364271</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.410800</td>\n      <td>1.328203</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.182500</td>\n      <td>1.296994</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.508300</td>\n      <td>1.281765</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.493500</td>\n      <td>1.275239</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=1.3744885444641113, metrics={'train_runtime': 542.3013, 'train_samples_per_second': 1.66, 'train_steps_per_second': 0.83, 'total_flos': 2182898237313024.0, 'train_loss': 1.3744885444641113, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:44:46.197645Z","iopub.execute_input":"2024-08-07T19:44:46.198698Z","iopub.status.idle":"2024-08-07T19:45:05.834124Z","shell.execute_reply.started":"2024-08-07T19:44:46.198644Z","shell.execute_reply":"2024-08-07T19:45:05.833062Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410b5b6f21584069a4ac7063db353857"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/rajora/phi-3-mini-4k-canarim/commit/218aed6a9c40b9cd540fed7fa53cffd229868d70', commit_message='Upload model', commit_description='', oid='218aed6a9c40b9cd540fed7fa53cffd229868d70', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:46:33.746067Z","iopub.execute_input":"2024-08-07T19:46:33.746460Z","iopub.status.idle":"2024-08-07T19:46:36.571007Z","shell.execute_reply.started":"2024-08-07T19:46:33.746429Z","shell.execute_reply":"2024-08-07T19:46:36.570302Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.033 MB of 0.033 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▁▅▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█▃▆█</td></tr><tr><td>eval/steps_per_second</td><td>▁█▃▆█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▄▄▇▆▃▂▄▅▁▂▄▆▁▂▂▄▆▂▂▃▄▂▂▄▅▆▂▅▅▅▂▃▂▃█▂▃▄▇</td></tr><tr><td>train/learning_rate</td><td>▄███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▆▆▅▆▅▂▁▆▄▂▃▅▇▂▅▂▅▅▆▃▂▄▄▂▇▆▆▄▅▄▅▆▆▂▃█▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.27524</td></tr><tr><td>eval/runtime</td><td>21.6661</td></tr><tr><td>eval/samples_per_second</td><td>4.616</td></tr><tr><td>eval/steps_per_second</td><td>4.616</td></tr><tr><td>total_flos</td><td>2182898237313024.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>1.51368</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4935</td></tr><tr><td>train_loss</td><td>1.37449</td></tr><tr><td>train_runtime</td><td>542.3013</td></tr><tr><td>train_samples_per_second</td><td>1.66</td></tr><tr><td>train_steps_per_second</td><td>0.83</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">tough-wood-1</strong> at: <a href='https://wandb.ai/rajora/Fine-tune%20Phi%203%20Mini%204k%20on%20Portuguese%20Dataset/runs/10p2ude2' target=\"_blank\">https://wandb.ai/rajora/Fine-tune%20Phi%203%20Mini%204k%20on%20Portuguese%20Dataset/runs/10p2ude2</a><br/> View project at: <a href='https://wandb.ai/rajora/Fine-tune%20Phi%203%20Mini%204k%20on%20Portuguese%20Dataset' target=\"_blank\">https://wandb.ai/rajora/Fine-tune%20Phi%203%20Mini%204k%20on%20Portuguese%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240807_193132-10p2ude2/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Descubra a probabilidade de o seguinte texto ser lido por alguém que não está familiarizado com o tópico. saída alta, média, baixa. Entrada: Uma boa maneira de começar a resolver um problema é pensar sobre o que você já sabe sobre o problema.\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:47:56.269312Z","iopub.execute_input":"2024-08-07T19:47:56.269699Z","iopub.status.idle":"2024-08-07T19:48:02.173905Z","shell.execute_reply.started":"2024-08-07T19:47:56.269670Z","shell.execute_reply":"2024-08-07T19:48:02.172973Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\nMédio. O texto é sobre resolver um problema, que é um tópico geral. No entanto, ele também contém alguns termos específicos, como \"resolver um problema\" e \"pensar sobre o que você já sabe sobre o problema\", que podem ser dif\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Merging","metadata":{}},{"cell_type":"code","source":"base_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:53:40.188071Z","iopub.execute_input":"2024-08-07T19:53:40.188785Z","iopub.status.idle":"2024-08-07T19:53:48.928997Z","shell.execute_reply.started":"2024-08-07T19:53:40.188750Z","shell.execute_reply":"2024-08-07T19:53:48.928248Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"774818635cca4c9ea6176ce1b2de590e"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366ac7a70b68412587062d582f948d73"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee50bb59ab004a97bbd5647fbe162d95"}},"metadata":{}}]},{"cell_type":"code","source":"base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T19:57:02.012191Z","iopub.execute_input":"2024-08-07T19:57:02.012565Z","iopub.status.idle":"2024-08-07T19:57:02.026500Z","shell.execute_reply.started":"2024-08-07T19:57:02.012539Z","shell.execute_reply":"2024-08-07T19:57:02.025601Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32013, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-31): 32 x Phi3DecoderLayer(\n            (self_attn): Phi3Attention(\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n              (rotary_emb): Phi3RotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=3072, out_features=32013, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_merging = '/kaggle/working/phi-3-mini-4k-canarim'\n\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(base_model_reload, model_merging)\n\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:01:51.911323Z","iopub.execute_input":"2024-08-07T20:01:51.912239Z","iopub.status.idle":"2024-08-07T20:01:52.385362Z","shell.execute_reply.started":"2024-08-07T20:01:51.912186Z","shell.execute_reply":"2024-08-07T20:01:52.384357Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:01:59.581388Z","iopub.execute_input":"2024-08-07T20:01:59.582025Z","iopub.status.idle":"2024-08-07T20:01:59.590392Z","shell.execute_reply.started":"2024-08-07T20:01:59.581982Z","shell.execute_reply":"2024-08-07T20:01:59.589472Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Phi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32013, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32013, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Descubra a probabilidade de o seguinte texto ser lido por alguém que não está familiarizado com o tópico. saída alta, média, baixa. Entrada: Uma boa maneira de começar a resolver um problema é pensar sobre o que você já sabe sobre o problema.\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:02:55.197444Z","iopub.execute_input":"2024-08-07T20:02:55.197824Z","iopub.status.idle":"2024-08-07T20:02:58.244388Z","shell.execute_reply.started":"2024-08-07T20:02:55.197793Z","shell.execute_reply":"2024-08-07T20:02:58.243434Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","output_type":"stream"},{"name":"stdout","text":"\nMédia. O texto é sobre resolver um problema, que é um tópico geral que pode ser compreendido por alguém que não está familiarizado com o assunto. No entanto, o texto também usa algumas palavras específicas, como \"resolver\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"phi-3-mini-4k-canarim\")\ntokenizer.save_pretrained(\"phi-3-mini-4k-canarim\")","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:03:44.453602Z","iopub.execute_input":"2024-08-07T20:03:44.453988Z","iopub.status.idle":"2024-08-07T20:04:12.632171Z","shell.execute_reply.started":"2024-08-07T20:03:44.453949Z","shell.execute_reply":"2024-08-07T20:04:12.631259Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"('phi-3-mini-4k-canarim/tokenizer_config.json',\n 'phi-3-mini-4k-canarim/special_tokens_map.json',\n 'phi-3-mini-4k-canarim/tokenizer.model',\n 'phi-3-mini-4k-canarim/added_tokens.json',\n 'phi-3-mini-4k-canarim/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"phi-3-mini-4k-canarim\", use_temp_dir=False)\ntokenizer.push_to_hub(\"phi-3-mini-4k-canarim\", use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-07T20:04:16.538141Z","iopub.execute_input":"2024-08-07T20:04:16.538567Z","iopub.status.idle":"2024-08-07T20:07:30.093763Z","shell.execute_reply.started":"2024-08-07T20:04:16.538536Z","shell.execute_reply":"2024-08-07T20:07:30.092778Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80eff375a11a45b8a6f527cdda8ff200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b47d3c0d5b454b979f80eb2205e6e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc0089e001a74b6b99ded22ffaee0d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21be78c9d06b4f0b8af30c3a3b2ada20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58d1126391204c8ab1a0c30a634a0e16"}},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/rajora/phi-3-mini-4k-canarim/commit/f78602ba3193ec087c784aa0e6e2c7bb4204964a', commit_message='Upload tokenizer', commit_description='', oid='f78602ba3193ec087c784aa0e6e2c7bb4204964a', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Converting Safetensors to GGUF model format","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:14:24.603043Z","iopub.execute_input":"2024-08-08T17:14:24.603931Z","iopub.status.idle":"2024-08-08T17:14:28.668870Z","shell.execute_reply.started":"2024-08-08T17:14:24.603898Z","shell.execute_reply":"2024-08-08T17:14:28.667676Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 1074, done.\u001b[K\nremote: Counting objects: 100% (1074/1074), done.\u001b[K\nremote: Compressing objects: 100% (813/813), done.\u001b[K\nremote: Total 1074 (delta 258), reused 671 (delta 214), pack-reused 0\u001b[K\nReceiving objects: 100% (1074/1074), 17.68 MiB | 17.74 MiB/s, done.\nResolving deltas: 100% (258/258), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 conda run -n base make -j > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:14:34.637134Z","iopub.execute_input":"2024-08-08T17:14:34.638106Z","iopub.status.idle":"2024-08-08T17:24:20.018441Z","shell.execute_reply.started":"2024-08-08T17:14:34.638062Z","shell.execute_reply":"2024-08-08T17:24:20.017068Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/working/llama.cpp\n","output_type":"stream"}]},{"cell_type":"code","source":"!python convert_hf_to_gguf.py /kaggle/input/phi-3-mini-4k-canarim/pytorch/default/1/phi-3-mini-4k-canarim \\\n    --outfile /kaggle/working/phi-3-mini-4k-canarim.gguf \\\n    --outtype f16","metadata":{"execution":{"iopub.status.busy":"2024-08-08T17:29:15.207088Z","iopub.execute_input":"2024-08-08T17:29:15.208049Z","iopub.status.idle":"2024-08-08T17:30:11.177793Z","shell.execute_reply.started":"2024-08-08T17:29:15.208002Z","shell.execute_reply":"2024-08-08T17:30:11.176565Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Writing: 100%|███████████████████████████| 7.64G/7.64G [00:49<00:00, 154Mbyte/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n%cd /kaggle/working/llama.cpp\n!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n!LLAMA_CUDA=1 conda run -n base make -j > /dev/null","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api = HfApi()\napi.upload_file(\n    path_or_fileobj=\"/kaggle/working/phi-3-mini-4k-canarim.gguf\",\n    path_in_repo=\"phi-3-mini-4k-canarim.gguf\",\n    repo_id=\"rajora/phi-3-mini-4k-canarim\",\n    repo_type=\"model\",\n)","metadata":{},"execution_count":null,"outputs":[]}]}